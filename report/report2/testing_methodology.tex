\documentclass[10pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\begin{document}

\end{document}Further to the discussion of development methodologies in the previous report,
the group had identified a number of ideologies to be adopted. A test driven
approach to development was one of those concepts to be applied when possible.
This section will expand further on the testing approach being applied to
different aspects of the development. 

\subsection{Maximum value}
The team is approaching testing in such a manor as to extract the `maximum
value? from testing efforts, this is primarily due to the goals and timescale
of the project. During the life of this project the team is not aiming to
achieve 100\% test coverage, but to concentrate testing efforts in areas with
the greatest reward.

With this in mind, aspects of the development will strive for a good level of
test coverage. These areas include the mobile application, web-server
application and data scraping scripts. It is hoped that having good coverage of
these aspects will help to speed up the development effort, by way of reducing
the time necessary to track down bugs and additionally to offer a higher
quality user experience.

\subsection{Functional testing}
To test boundaries between systems, functional testing is being applied. This
enables testing of  the inputs and outputs of the system as a whole conform to
the expected responses. Ensuring these boundaries behave correctly is an
important step in ensuring stability as a whole.

The biggest of these boundaries is between the mobile application and the back
end service, where communication occurs over a REST API. The functional testing
of this interface is being performed using the command line tool ?cURL? which
was useful for initial development. But as development progresses the team is
investigating moving to Apache JMeter to provide a better testing platform.
JMeter also offers a simpler interface for creating tests and has the added
advantage of being able to stress test the service, something that has been
discussed at our supervisor meetings.

\subsection{Unit testing}
Unit testing is a valued technique as it not only provides a mechanism for
exercising software modules through its interfaces, but importantly encourages
good software design principles such as modularity and low coupling. The
components of this project which are expected to have the largest percentage of
code coverage are the tweet processing pipeline, REST API server and mobile
application.

Separate test suites are under concurrent development for the Python and Java
portions of the project. Currently we are striving to create these tests in a
test driven manor, but this practice can be difficult to uphold - particularly
with pending deadlines.

\subsection{Classifier verification}
Machine learning algorithms induce classifiers that depend on the training set,
and there is a need for evaluation and statistical testing to assess the
expected error rate of a classification algorithm, and even compare the
expected error rates of two classification algorithms to be able to say which
one is better. Evaluation can also be used as a guide for future improvements
on the model. In order to evaluate the classifier several techniques have been
used. The first technique is to generate a test set of tweets which their
labels are already known. This test set has to be distinct from the train set
which has been used to train the classifier. Afterward this test set is being
classified by the classifier and the labels, that it decides, are being
compared with their correct labels. The second technique is to calculate the
accuracy of the classifier which measures the percentage of inputs in the test
set that the classifier correctly labelled.

However more techniques will be used, when the dataset will be further
increased, in order to get more accurate evaluations and avoid possible
`overfitting'. The first of these methods is the K-Fold Cross Validation. The
dataset is split each time into K equally sized subsets, training and testing
datasets, and then in n-th iteration (n=1..k) the n-th subset(testing set) is
used for testing the classifier that has been built on all other remaining
subsets. To generate multiple samples from a single sample, an alternative to
cross-validation is the Bootstrap that generates new samples by drawing
instances from the original samples (one subset of samples from train set and one from test set)
 with replacement. Another way to evaluate our classifier is the
Confusion Matrix which is a visualization tool typically used to present the
results attained by a learner. Each column of the matrix represents the
instances in a predicted class, while each row represents the instances in an
actual class.For this purpose, the NLTK package provides the function
nltk.ConfusionMatrix(). Finally, the Precision and Recall Rates can be
calculated in order to ensure the results from the previous method. The recall
and the precision can be derived easily from the confusion matrix.
