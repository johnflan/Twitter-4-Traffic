\subsection{Clustering}
The behaviour of users of social media in a specific timeframe of each day of
the week tend to follow a pattern. Especially if one looks at results of social
media usage considering the location from where social data were submitted,
they can see that pattern. For this project, the interest is on traffic social
data.
Using that theory, traffic disruptions can be detected through the unusual use
of social media. This way the detected data will be about unusual traffic, so
it would be able to show an accident or a closed road etc. In this project and
because people don't tend to use social media that much about traffic yet we
tested that theory using general social data and not only about traffic.
In order to do that, static clusters had to be created first. These clusters
represent the "normal" behaviour of the social media users. For all clustering
k-means was used due to its simplicity. The static data used in the project are
from a time frame of every day. After that a timeslice data are computed for
the same timeframe of a specific day.
The static data and the timeslice data are then compared, because unusual
behaviour is what is important. For the comparison, each cluster is considered
to be a Gaussian distribution. The Kullback-Leibler divergence is then applied
to the timeslice clusters comparing them to the static clusters. The value
returned from that function is a real number larger or equals to zero. The
closest this number is to zero, the more the distributions are alike.

\subsection{Gamification}

\subsection{Further enhanced on the classification}
There are multiple ways that can improve the classifier. However, because of the strictly deadline of the project the time wasn’t enough to test all the possible improvements. Therefore the team decided to focus on the other parts of the project. One way to improve classification performance is to combine several classifiers. This can be easily be done by letting the multiple classifiers to use voting, and choose whichever label gets the most votes. For this style of voting, it's best to have an odd number of classifiers so that there are no ties. The individual classifiers should also use different algorithms. For example the Naïve Bayes, the SVM and another classifier e.g a Decision Tree classifier can vote for a tweet and an algorithm can be used in order to classify the tweet accordingly.  Another way to enhance the classifier is to manually label more data and train the classifier from the beginning. Finally, the inclusion of trigrams collocations could increase the accuracy of the classification.  

\subsection{Analysis of other sources}
As the aim of this project is providing information to the user about traffic disruptions, more sources can 
easily be used for acquiring more information about disruptions in London and other areas. Such examples 
of sources would be news reports published in various websites. These reports could include accidents, general traffic or even 
bad road conditions due to the weather. Moreover, there exist TfL Twitter bots that frequently tweet
about traffic disruptions. Another source that can be used is Facebook. Data regarding traffic can be
extracted from posts, comments or even public groups whose current location is London.

\subsection{Enhanced tweet geocoding}
This project can store and provide geolocations of tweets for users to see where the traffic tweets are from. For some reasons, mobile users cannot tweet the right position where the disruption is. Geolocation needs be double-checked before it makes sense. What's more, some geolocations of street stored in the database will change in the future. Those data should be checked and updated periodically via connecting Google maps geocoding. Algorithm for extracting street address from text need to be optimized to get more accurate results which affect the corresponding coordination sent from Google Map.
