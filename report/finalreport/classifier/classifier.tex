Document classification is a way to categorize documents or pieces of text. By examining the words in a piece of text, classifiers can decide, after the training what class label to assign to it. A binary classifier decides between two labels, such as traffic or non-traffic. The text can either be one label or the other, but not both, whereas a multi-label classifier can assign one or more labels to a piece of text. For the particular project, because it was only required to label the tweets as traffic or as non-traffic, the binary classifier has been chosen.

Two approaches for the automatic document classification exist: supervised document classification and unsupervised classification (clustering) \cite{Liu}.  The supervised classification requires training data in order to be able to classify the data. This classification works by learning from labelled feature sets, or training data, to later classify an unlabelled text, or a feature set. A feature set is basically a key-value mapping of feature names to feature values. In the case of text classification, the feature names are usually words or blocks of words, and the values are all True. As the texts may have unknown words, and the number of possible words may be very large, words that do not occur in the text are omitted, instead of being included in a feature set with the value False.

On the other hand, unsupervised classification instead of training data uses several algorithms, mainly clustering algorithms, to classify text. The unsupervised classification starts from a certain point and its algorithms try in iterative ways to reach a stable configuration that makes sense. Therefore, it may require a large amount of time to reach this configuration and the time was of the essence because of the strict project time restrictions. Additionally, the results of the unsupervised classification vary widely and may be completely off if the first steps are wrong.

The team decided to adopt the supervised classification because it is a more stable technique and it can be used as soon as the classifier is trained. It is only needed to gather the training data, train the classifier and then the classifier is ready to start classifying. After the decision of the classifier, it was necessary to apply several text normalizations on the data. 

\subsection{Text Normalization}
Text normalization is the way to eliminate the low information features, the words that don't offer useful information to the classifier. The elimination of those low information features provides to the model clarity by removing noisy data. Additionally it reduces the possibility of over-fitting by training the classifier with unnecessary data. By using the higher information features, the performance is being increased while the size of the model is being decreased, which results in less memory usage along with faster training and classification. This normalization was being applied on the labelled tweets before the classifier training and is continue being applied on the newly fetched tweets as well. Using this pre-processing on the tweets the accuracy of the classifier has been increased. 

The below text normalazation techniques are being applied on the tweets before the classification.
\begin{itemize}
\item Convert URL links to a more redable way.\\ 
This has been done by using a regular expression to recognize the link. After that the domain is being extracted from the URL and it replaces the link itself. 
\item Convert emoticons into words.\\ 
The next step is to replace the emoticons with a global name so they will not be deleted during the punctuation removal. That is important because the emoticons offer useful information during the classification. For this reason, the team has integrated a script which is responsible on finding those emoticons, assign them to a group of emoticons and replace them with the name of the group. Four groups of emoticons have been created:Very Happy, Happy, Sad, Very Sad. 
\item Remove the punctuations.\\
The team has accomplished that by creating a regular expression which represents all the possible punctuations. 
\item Convert all letters to lower case.
\item Tokenise the tweet into words.
\item Group together the different inflected forms of a word into a single item.\\
This has been done by applying lemmanization on the tokenised text. Lemmanization removes and replaces word suffixes to arrive at a common root form of the word in order to group up the common words. This method has been chosen over the stemming, because lemma is a canonical set of words, instead of the stem which in many cases is not a real world. 
\item Find the bigrams collocations.\\
Bigrams are less common than most individual words, so including them in training data increases the classifier accuracy. 
\end{itemize}

\subsection{Classifiers}
The next step was to investigate several supervised classification algorithms. The methods Support Vector Machines and Naive Bayes have been chosen for this purpose. Those two algorithms have been selected because of a number of factors. Firstly, Naive Bayes train very quickly since it requires only a single pass on the data to compute the normal probability density function. It also requires little storage space during the training and classification stages: the strict minimum is the memory needed to store the prior and conditional probabilities. Additionally Naive Bayes is very transparent, as it is easily grasped by users and it provides a discrete probability for each tweet helping the ranking of the tweets. Naive Bayes is considered to have high bias, because it assumes that the dataset under consideration can be summarized by a single probability distribution and that Naive Bayes model is sufficient to discriminate between classes. This high bias usually generates simple, highly constrained models. On the other hand, SVM is considered to be one of the most accurate classifier. However for the SVM, a large sample size is required in order to achieve its maximum prediction accuracy whereas Naive Bayes may need a relatively small dataset.

\subsubsection{Naive Bayes}
Given a set of objects, each of which belongs to a known class, and each of which has a known vector of variables, the aim is to construct a rule which will allow the assigning of future objects to a class, given only the vectors of variables describing the future objects. Problems of this kind, called problems of supervised classification, are ubiquitous, and many methods for constructing such rules have been developed. One method is the Naive Bayes Reasoning. This is a well-established Bayesian method primarily formulated for performing classification tasks. Given its simplicity Naive Bayes models are effective classification tools that are easy to use and interpret. Naive Bayes is particularly appropriate when the dimensionality of the independent space. For the reasons given above, Naive Bayes can often outperform other more sophisticated classification methods. A variety of methods exist for modelling the conditional distributions of the inputs including normal, lognormal, gamma, and Poisson \cite{Barber}. 

This classifier has been created using the "Bag of Words" model and the NLTK suites of libraries. NLTK is the Natural Language Toolkit, a comprehensive Python library for natural language processing and text analytics. It was decided to use the Natural Language Toolkit because of its simplicity, consistency, extensibility and its modularity. Additionally some of the members had already experience with it and they were aware of its accuracy. Furthermore, it was decided the usage of the "Bag of Words" feature extraction because it's a very accurate method, especially for binary classification. Text feature extraction is the process of transforming what is essentially a list of words into a feature set that is usable by a classifier. The NLTK Naive Bayes classifier expects dictionary style feature sets, so the text should be transformed into a dictionary. The "Bag of Words" model is a well-known method for representing documents, which ignores the word orders \cite{Bird}\cite{Perkins}. It constructs a word dictionary from all the words of an instance where every word gets the value True. An instance is a single feature set. It represents a single occurrence of a combination of features. A labelled feature set is in fact an instance with a known class label that we can use for training or evaluation.

As it has discussed previously, for the training of the classifier it's required a labelled data. To accomplish that, a simple script for manually labelling was created. This script was being executed on a temporary table on the database which was containing raw, unlabelled, tweets. During the execution it was presenting random tweets from this table and the user was able to press four buttons in order to label the tweets as traffic (personalized tweets about traffic), non-traffic, unclear and bot (tweets about traffic from official sites). After the gathering of one and a half thousand of traffic-about tweets, the table in which the labelled tweets were being stored was used to train the classifier.

\subsubsection{Support Vector Machines}
The second supervised learning method that it was fully integrated and tested is the Support Vector Machines (SVM). This is a method that performs regression and classification tasks by constructing nonlinear decision boundaries. Because of the nature of the feature space in which these boundaries are found, Support Vector Machines can exhibit a large degree of flexibility in handling classification and regression tasks of varied complexities. There are several types of Support Vector models including linear, polynomial, RBF, and sigmoid.

In this project PyML\cite{website:pyml} was used to implement SVM
classification. The results of this algorithm did not show much improvement
compared to Naive Bayes classification. Also the library seems to be at an
early stage which makes it more difficult to use. Since the accuracy does not
change, the use of Naive Bayes was decided by the group.
