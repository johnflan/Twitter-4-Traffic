Document classification is a way to categorize documents or pieces of text. By examining the words in a piece of text, classifiers can decide, after the training, what class label to assign to it. A binary classifier decides between two labels, such as traffic or non-traffic. The text can either be one label or the other, but not both, whereas a multi-label classifier can assign one or more labels to a piece of text. For the particular project, because it was only required to label the tweets as traffic or as no-traffic, the binary classifier has been chosen.

Two approaches for the automatic document classification exist: supervised document classification and unsupervised classification (clustering).  The supervised classification requires a training data in order to be able to classify the data. This classification works by learning from labelled feature sets, or training data, to later classify an unlabelled text, or a feature set. A feature set is basically a key-value mapping of feature names to feature values. In the case of text classification, the feature names are usually words or blocks of words, and the values are all True. As the texts may have unknown words, and the number of possible words may be very large, words that don't occur in the text are omitted, instead of including them in a feature set with the value False.

On the other hand, clustering instead of a training data uses several algorithms, mainly clustering algorithms, to classify a text. The unsupervised classification starts from a certain point and its algorithms try in iterative ways to reach a stable configuration that makes sense. Therefore it may require a large amount of time to reach this configuration and the time was on the essence because of the strict project time restrictions. Additionally, the results of the unsupervised classification vary widely and may be completely off if the first steps are wrong.

The team decided to adopt the supervised classification because it’s a more stable technique and it can be used as soon as the classifier is been trained. It is only needed to gather the training data, train the classifier and the classifier is ready to start classifying. The next step was to investigate several supervised classification algorithms. 

The team has examined the classification of the tweets with the methods Support Vector Machines and Naive Bayes. Those two algorithms have been selected cause of a number of reasons. Firstly, Naive Bayes train very quickly since it requires only a single pass on the data to compute the normal probability density function. It also requires little storage space during the training and classification stages: the strict minimum is the memory needed to store the prior and conditional probabilities. Additionally Naive Bayes is very transparent, as it is easily grasped by users and it provides a discrete probability for each tweet helping the ranking of the tweets. Naive Bayes is considered to have high bias, because it assumes that the dataset under consideration can be summarized by a single probability distribution and that Naive Bayes model is sufficient to discriminate between classes. This high bias usually generates simple, highly constrained models. On the other hand, SVM is considered to be one of the most accurate classifier. However for the SVM, a large sample size is required in order to achieve its maximum prediction accuracy whereas Naïve Bayes may need a relatively small dataset.

\subsection{Text Normalization}
After the decision of the classifier it was necessary to apply several text normalizations on the data. Text normalization is the way to eliminate the low information features, the words that don’t offer useful information to the classifier. The elimination of those low information features provides to the model clarity by removing noisy data. Additionally it reduces the possibility of over-fitting by training the classifier with unnecessary data. By using the higher information features, the performance is being increased while the size of the model is being decreased, which results in less memory usage along with faster training and classification. This normalization was being applied on the labelled tweets before the classifier training and is continue being applied on the newly fetched tweets as well. Using this pre-processing on the tweets the accuracy of the classifier has been increased. 

The below text normalazation techniques are being applied on the tweets before the classification.
\begin{itemize}
\item Convert URL links to a more redable way. 
This has been done by using a regular expression to recognize the link. After that the domain is being extracted from the URL and it replaces the link itself. 
\item Convert emoticons into words. 
The next step is to replace the emoticons with a global name so they will not be deleted during the punctuation removal. That is important because the emoticons offer useful information during the classification. For this reason, the team has integrated a script which is responsible on finding those emoticons, assign them to a group of emoticons and replace them with the name of the group. Four groups of emoticons have been created:Very Happy, Happy, Sad, Very Sad. 
\item Remove the punctuations.
The team has accomplished that by creating a regular expression which represents all the possible punctuations. 
\item Convert all letters to lower case.
\item Tokenise the tweet into words.
\item Group together the different inflected forms of a word into a single item.
This has been done by applying lemmanization on the tokenised text. Lemmanization removes and replaces word suffixes to arrive at a common root form of the word in order to group up the common words. This method has been chosen over the stemming, because lemma is a canonical set of words, instead of the stem which in many cases is not a real world. 
\item Find the bigrams collocations.
Bigrams are less common than most individual words, so including them in training data increases the classifier accuracy. 
\end{itemize}

\subsection{Classifiers}

\subsubsection{Naive Bayes}

\subsubsection{Support Vector Machines}
