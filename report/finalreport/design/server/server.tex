\subsubsection{Data acquisition}
The sources that we collect data from are Transport for London (TfL) and Twitter.

The TfL website provides different types of data feeds that can be used to obtain information relevant to traffic. 
One of the TfL data feeds we use is the "live traffic disruptions". This provides information about traffic disruption events in the area of London. This information contains details about the severity of the event, the type (e.g. road works, signal failure, accident), the location and an estimated time for the event's end.

To provide a more complete view around the area of the event we also store the "live traffic camera images" feed from TfL. This provides urls to pictures taken by traffic cameras within the last 15 minutes as well as the location of the camera and the timestamp when the picture was taken.

These feeds are returned in XML format and need to be parsed before we store them in the database. To achieve this, we use the xml.dom.minidom python module. This can parse the XML file to a Document Object Model (DOM). A DOM tree can be accessed by using functions to call each field of the XML object that was used to create it.

For Twitter we defined the area around London that we are interested to collect tweets from. In addition to this, we used a search query with the words that we want the results from Twitter to contain. This query is:  «traffic OR accident OR tailback OR gridlock OR m25 OR standstill OR road OR street OR stuck OR car OR bus OR delay». 

In contrast to the TfL, the Twitter data are returned in a JSON format. For this reason, we used the python-twitter API which saves the JSON information into classes and variables that can then be easily accessed and stored in the database.

\subsubsection{Data analysis}
\subsubsection{Storage}
For the storage of data we use a PostgreSQL database. Because of the high amount of database queries we needed to use an efficient way to store and analyze the information we acquired from the tfl feeds and twitter. For this reason, we decided to use a Geographic Information System called PostGIS. This allowed us to store geolocations (longitude and latitude) as points.

With the use of PostGIS queries to the database became easier and more efficient. The use of functions provided by PostGIS, such as ST\_DWithin to find all points around a route or a point within a radius, or ST\_Distance to find the distance between two points, made all queries simpler. In addition, the main advantage of PostGIS is that it uses generalized search trees (GiST) to index the geometries. 

A GiST like a B-tree uses key-pointer pairs. The difference with B-trees is that a GiST key is a user-defined data type. This allows different types of operations, rather than simple comparisons, such as nearest-neighbor searches and statistical approximations over large data sets. In PostGIS, the GiST is used for spatial indexing by allowing the index to use a bounding box for each geometry (e.g. line) instead of storing the whole geometry in it. Therefore, with bounding box comparison, instead of comparing geometries, functions such as ST\_DWithin are made more efficient.

For the project we used the following tables:
\begin{itemize}
\item Tables used by the final application
  \begin{itemize}
  \item tfl: The current tfl disruption events
  \item archive: To store old tfl disruption events for further analysis
  \item tflcameras: The current camera pictures' url
  \item tweets: To store the traffic tweets acquired from twitter
  \item geolookup: Used to store addresses with their SoundEX value and their geolocation which is acquired from Google Maps
  \item tweets\_metrics: Used to store different types of metrics for the tweets
  \end{itemize}
\item Tables used to train the classifier
  \begin{itemize}
  \item labelled\_tweets: To store manually labelled tweets as traffic or non-traffic
  \item stop\_words: Words that can be removed before data analysis
  \end{itemize}
\item Tables used by PostGIS
  \begin{itemize}
  \item geography\_columns: This is a view that shows all the columns of the database that use geography points
  \item spatial\_ref\_sys: A list of spatial reference systems and details to transform between them
  \end{itemize}
\end{itemize}

\subsubsection{Interface}

Before the actual server was implemented, the creation of a mock server api was essential. This 
provided us with the ability to seperate the work on the application and server design from the very 
beginning of the project. At this stage, the rest endpoints had to be defined in order to agree on the 
data format that was transferred between the server and the client. The next thing was creating mocked 
JSON data for responding to the requests. Such data included fixed sets of disruptions, tweets and 
cameras. 





